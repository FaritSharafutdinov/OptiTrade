import os
import sys
from pathlib import Path
from typing import List, Any, Dict, Optional, Union
import numpy as np
import pandas as pd
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import logging

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ RL_algorithms –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
rl_algorithms_path = Path(__file__).parent.parent / "RL_algorithms"
sys.path.insert(0, str(rl_algorithms_path))
sys.path.insert(0, str(rl_algorithms_path / "algorithms_training"))

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="RL Model Service")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–µ–π
rl_models = {}  # –°–ª–æ–≤–∞—Ä—å –≤—Å–µ—Ö –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: {"ppo": model, "a2c": model, "sac": model}
current_model_type = None  # –¢–µ–∫—É—â–∞—è –∞–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å
model_env = None
observation_cache = {}  # –ö—ç—à –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –ø–æ —Å–∏–º–≤–æ–ª–∞–º

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
MODEL_PATH = os.getenv("MODEL_PATH", None)
MODEL_TYPE = os.getenv("MODEL_TYPE", "ppo")  # ppo, a2c, sac
USE_RL_MODEL = os.getenv("USE_RL_MODEL", "true").lower() == "true"
LAZY_LOAD_MODELS = os.getenv("LAZY_LOAD_MODELS", "false").lower() == "true"  # –ó–∞–≥—Ä—É–∂–∞—Ç—å –º–æ–¥–µ–ª–∏ –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é


class Observation(BaseModel):
    features: List[float]
    symbol: str
    timestamp: str
    current_price: Optional[float] = None
    volume: Optional[float] = None
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ—Ä—Ç—Ñ–µ–ª—è –¥–ª—è RL –º–æ–¥–µ–ª–∏
    position: Optional[float] = 0.0
    position_size: Optional[float] = 0.1
    equity: Optional[float] = 10000.0


def load_single_model(model_type_name: str) -> Optional[Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–¥–Ω–æ–π RL –º–æ–¥–µ–ª–∏ –ø–æ —Ç–∏–ø—É"""
    try:
        from stable_baselines3 import PPO, SAC, A2C
        
        # –ò—â–µ–º –º–æ–¥–µ–ª—å –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –ø–∞–ø–∫–µ
        model_dir = rl_algorithms_path / "models" / model_type_name.upper()
        model_files = list(model_dir.glob(f"{model_type_name}_baseline.zip"))
        if not model_files:
            logger.warning(f"–ú–æ–¥–µ–ª—å {model_type_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {model_dir}")
            return None
        
        model_path = model_files[0]
        logger.info(f"Loading {model_type_name.upper()} model from {model_path}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        model_classes = {
            "ppo": PPO,
            "a2c": A2C,
            "sac": SAC
        }
        
        ModelClass = model_classes.get(model_type_name.lower(), None)
        if not ModelClass:
            logger.error(f"Unknown model type: {model_type_name}")
            return None
        
        model = ModelClass.load(str(model_path))
        logger.info(f"‚úÖ {model_type_name.upper()} model loaded successfully")
        return model
        
    except Exception as e:
        logger.error(f"Error loading {model_type_name} model: {e}", exc_info=True)
        return None


def load_all_models():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö RL –º–æ–¥–µ–ª–µ–π"""
    global rl_models, current_model_type
    
    if not USE_RL_MODEL:
        logger.info("RL models disabled, using simple mode")
        return
    
    available_types = ["ppo", "a2c", "sac"]
    loaded_count = 0
    
    for model_type_name in available_types:
        model = load_single_model(model_type_name)
        if model:
            rl_models[model_type_name.lower()] = model
            loaded_count += 1
    
    if loaded_count > 0:
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: MODEL_TYPE –∏–∑ env, –∑–∞—Ç–µ–º –ø–µ—Ä–≤–∞—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è)
        current_model_type = MODEL_TYPE.lower() if MODEL_TYPE.lower() in rl_models else list(rl_models.keys())[0]
        logger.info(f"‚úÖ Loaded {loaded_count} models. Active model: {current_model_type.upper()}")
    else:
        logger.warning("No RL models loaded, will use simple mode")
        current_model_type = None


def load_rl_model():
    """–ó–∞–≥—Ä—É–∑–∫–∞ RL –º–æ–¥–µ–ª–µ–π (backward compatibility)"""
    global current_model_type
    
    if LAZY_LOAD_MODELS:
        # Lazy loading - –∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—É—â—É—é –º–æ–¥–µ–ª—å
        model = load_single_model(MODEL_TYPE)
        if model:
            rl_models[MODEL_TYPE.lower()] = model
            current_model_type = MODEL_TYPE.lower()
        else:
            current_model_type = None
    else:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
        load_all_models()


@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ —Å–µ—Ä–≤–∏—Å–∞"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ Model Service...")
    load_rl_model()


@app.get("/health")
async def health():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    return {
        "status": "ok",
        "models_loaded": list(rl_models.keys()),
        "active_model": current_model_type,
        "total_models": len(rl_models),
        "use_rl_models": USE_RL_MODEL
    }


def simple_predict(features: List[float]) -> Dict[str, Any]:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –±–µ–∑ RL –º–æ–¥–µ–ª–∏ (fallback)"""
    if not features:
        return {"action": "HOLD", "confidence": 0.5, "score": 0.0}
    
    # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É–º–º—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    score = sum(features) / len(features) if features else 0.0
    
    if score > 0.3:
        action = "BUY"
        confidence = min(0.95, 0.5 + abs(score) * 0.5)
    elif score < -0.3:
        action = "SELL"
        confidence = min(0.95, 0.5 + abs(score) * 0.5)
    else:
        action = "HOLD"
        confidence = 0.6
    
    return {
        "action": action,
        "confidence": round(confidence, 2),
        "score": round(float(score), 4)
    }


def get_active_model():
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –∞–∫—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å"""
    if current_model_type and current_model_type in rl_models:
        return rl_models[current_model_type]
    elif len(rl_models) > 0:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å
        return list(rl_models.values())[0]
    return None


def rl_model_predict(obs: Observation, model_type_override: Optional[str] = None) -> Dict[str, Any]:
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RL –º–æ–¥–µ–ª–∏"""
    global observation_cache
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫—É—é –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
    active_model = None
    used_model_type = current_model_type
    
    if model_type_override and model_type_override.lower() in rl_models:
        active_model = rl_models[model_type_override.lower()]
        used_model_type = model_type_override.lower()
    else:
        active_model = get_active_model()
    
    if active_model is None:
        return simple_predict(obs.features)
    
    try:
        # –î–ª—è RL –º–æ–¥–µ–ª–∏ –Ω—É–∂–Ω–∞ –ø–æ–ª–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è –Ω–∞–±–ª—é–¥–µ–Ω–∏–π
        # –°–æ–∑–¥–∞–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è: (window_size, num_features)
        # –î–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è —Å–æ–∑–¥–∞–¥–∏–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º features –≤ numpy array
        feature_array = np.array(obs.features, dtype=np.float32)
        
        # –ï—Å–ª–∏ features –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –¥–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à
        window_size = 30  # –ö–∞–∫ –≤ –æ–±—É—á–µ–Ω–∏–∏
        num_base_features = 11  # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ —Å—Ä–µ–¥—ã
        
        # –°–æ–∑–¥–∞–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ
        # –î–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞ –Ω—É–∂–Ω–æ –∫—ç—à–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é, –Ω–æ –ø–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–π –º–æ–º–µ–Ω—Ç
        if len(feature_array) < num_base_features:
            # –î–æ–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
            feature_array = np.pad(
                feature_array, 
                (0, max(0, num_base_features - len(feature_array))),
                mode='constant'
            )[:num_base_features]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Ä—Ç—Ñ–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        portfolio_features = np.array([
            obs.position or 0.0,
            obs.position_size or 0.1,
            (obs.equity or 10000.0) / 10000.0,  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª
            0.0  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫ (–ø–æ–∫–∞ 0)
        ], dtype=np.float32)
        
        # –°–æ–∑–¥–∞–µ–º –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ: –ø–æ–≤—Ç–æ—Ä—è–µ–º —Ç–µ–∫—É—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è window_size
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–∞ –∏—Å—Ç–æ—Ä–∏—è, –Ω–æ –¥–ª—è –¥–µ–º–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        observation = np.zeros((window_size, num_base_features + 4), dtype=np.float32)
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∞–≥ —Ç–µ–∫—É—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        observation[-1, :num_base_features] = feature_array[:num_base_features]
        observation[-1, num_base_features:] = portfolio_features
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∫—ç—à, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏
        cache_key = obs.symbol
        if cache_key in observation_cache:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∏–∑ –∫—ç—à–∞
            cached_obs = observation_cache[cache_key]
            if len(cached_obs) >= window_size - 1:
                observation[:-1] = cached_obs[-(window_size-1):]
            elif len(cached_obs) > 0:
                observation[:len(cached_obs)] = cached_obs[-len(cached_obs):]
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à (—Ö—Ä–∞–Ω–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ window_size –Ω–∞–±–ª—é–¥–µ–Ω–∏–π)
        if cache_key not in observation_cache:
            observation_cache[cache_key] = []
        observation_cache[cache_key].append(observation[-1])
        if len(observation_cache[cache_key]) > window_size:
            observation_cache[cache_key] = observation_cache[cache_key][-window_size:]
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        action_array, _ = active_model.predict(observation, deterministic=True)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ RL –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç –±–µ–∫–µ–Ω–¥–∞
        # action_array: [target_position (-1..1), target_size (0.1..1.0)]
        target_position = float(action_array[0])
        target_size = float(action_array[1])
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
        if target_position > 0.3:
            action = "BUY"
            confidence = min(0.95, abs(target_position) * target_size)
        elif target_position < -0.3:
            action = "SELL"
            confidence = min(0.95, abs(target_position) * target_size)
        else:
            action = "HOLD"
            confidence = 1.0 - abs(target_position)
        
        return {
            "action": action,
            "confidence": round(confidence, 2),
            "score": round(float(target_position), 4),
            "position": round(target_position, 4),
            "size": round(target_size, 4),
            "model_type": used_model_type
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è RL –º–æ–¥–µ–ª–∏: {e}", exc_info=True)
        # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π —Ä–µ–∂–∏–º
        return simple_predict(obs.features)


class PredictRequest(BaseModel):
    features: List[float]
    symbol: str
    timestamp: str
    current_price: Optional[float] = None
    volume: Optional[float] = None
    position: Optional[float] = 0.0
    position_size: Optional[float] = 0.1
    equity: Optional[float] = 10000.0
    model_type: Optional[str] = None  # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –≤—ã–±—Ä–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å


@app.post("/predict")
async def predict(req: Dict[str, Any]) -> Dict[str, Any]:
    """
    –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–≤–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞:
    1. –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç: PredictRequest —Å –ø–æ–ª–µ–º model_type
    2. –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç: Observation –∏–ª–∏ dict (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
    
    –ï—Å–ª–∏ RL –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –µ—ë.
    –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –ª–æ–≥–∏–∫—É.
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - model_type: –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –≤—ã–±–∏—Ä–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å (ppo/a2c/sac)
    """
    try:
        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –æ–±–æ–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        if isinstance(req, dict):
            # –ï—Å–ª–∏ –ø—Ä–∏—à–µ–ª dict (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç backend)
            obs = Observation(**req)
            model_type_override = req.get('model_type')
        elif isinstance(req, Observation):
            # –ï—Å–ª–∏ –ø—Ä–∏—à–µ–ª Observation –Ω–∞–ø—Ä—è–º—É—é
            obs = req
            model_type_override = None
        else:
            # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç —á–µ—Ä–µ–∑ PredictRequest (Pydantic model)
            obs = Observation(
                features=req.features,
                symbol=req.symbol,
                timestamp=req.timestamp,
                current_price=req.current_price,
                volume=req.volume,
                position=req.position,
                position_size=req.position_size,
                equity=req.equity
            )
            model_type_override = getattr(req, 'model_type', None)
        
        active_model = get_active_model()
        if active_model is not None or (model_type_override and model_type_override.lower() in rl_models):
            result = rl_model_predict(obs, model_type_override)
        else:
            result = simple_predict(obs.features)
        
        return result
        
    except Exception as e:
        logger.error(f"Error processing request: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/predict/batch")
async def predict_batch(observations: List[Observation], model_type: Optional[str] = None) -> List[Dict[str, Any]]:
    """–ü–∞–∫–µ—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π"""
    results = []
    active_model = get_active_model()
    
    for obs in observations:
        try:
            if active_model is not None or (model_type and model_type.lower() in rl_models):
                result = rl_model_predict(obs, model_type)
            else:
                result = simple_predict(obs.features)
            results.append(result)
        except Exception as e:
            logger.error(f"Error predicting for {obs.symbol}: {e}")
            results.append({"action": "HOLD", "confidence": 0.0, "error": str(e)})
    
    return results


@app.post("/cache/clear")
async def clear_cache(symbol: Optional[str] = None):
    """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π"""
    global observation_cache
    if symbol:
        if symbol in observation_cache:
            del observation_cache[symbol]
            return {"status": "ok", "message": f"Cache cleared for {symbol}"}
        else:
            return {"status": "ok", "message": f"No cache found for {symbol}"}
    else:
        observation_cache.clear()
        return {"status": "ok", "message": "All cache cleared"}


# ========== Model Management API ==========

@app.get("/models")
async def list_models():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    available_models = []
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –µ—Å—Ç—å –Ω–∞ –¥–∏—Å–∫–µ
    model_types = ["ppo", "a2c", "sac"]
    for model_type_name in model_types:
        model_dir = rl_algorithms_path / "models" / model_type_name.upper()
        model_files = list(model_dir.glob(f"{model_type_name}_baseline.zip"))
        
        is_loaded = model_type_name.lower() in rl_models
        is_active = model_type_name.lower() == current_model_type
        
        available_models.append({
            "type": model_type_name.upper(),
            "available": len(model_files) > 0,
            "loaded": is_loaded,
            "active": is_active,
            "path": str(model_files[0]) if model_files else None
        })
    
    return {
        "available_models": available_models,
        "active_model": current_model_type,
        "total_loaded": len(rl_models)
    }


@app.post("/models/switch")
async def switch_model(request: Dict[str, str]):
    """–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å"""
    global current_model_type, rl_models
    
    model_type_requested = request.get("model_type", "").lower()
    
    if not model_type_requested:
        raise HTTPException(status_code=400, detail="model_type is required")
    
    # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è
    if model_type_requested in rl_models:
        current_model_type = model_type_requested
        logger.info(f"Switched to {model_type_requested.upper()} model")
        return {
            "status": "ok",
            "message": f"Switched to {model_type_requested.upper()} model",
            "active_model": current_model_type
        }
    
    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
    model = load_single_model(model_type_requested)
    if model:
        rl_models[model_type_requested] = model
        current_model_type = model_type_requested
        logger.info(f"Loaded and switched to {model_type_requested.upper()} model")
        return {
            "status": "ok",
            "message": f"Loaded and switched to {model_type_requested.upper()} model",
            "active_model": current_model_type
        }
    else:
        raise HTTPException(
            status_code=404,
            detail=f"Model {model_type_requested} not found or failed to load"
        )


@app.post("/models/load")
async def load_model(request: Dict[str, str]):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å (–±–µ–∑ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è)"""
    global rl_models
    
    model_type_requested = request.get("model_type", "").lower()
    
    if not model_type_requested:
        raise HTTPException(status_code=400, detail="model_type is required")
    
    if model_type_requested in rl_models:
        return {
            "status": "ok",
            "message": f"{model_type_requested.upper()} model already loaded",
            "loaded_models": list(rl_models.keys())
        }
    
    model = load_single_model(model_type_requested)
    if model:
        rl_models[model_type_requested] = model
        logger.info(f"Loaded {model_type_requested.upper()} model")
        return {
            "status": "ok",
            "message": f"{model_type_requested.upper()} model loaded successfully",
            "loaded_models": list(rl_models.keys())
        }
    else:
        raise HTTPException(
            status_code=404,
            detail=f"Model {model_type_requested} not found or failed to load"
        )


@app.post("/models/reload")
async def reload_model(request: Dict[str, str]):
    """–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å"""
    global rl_models, current_model_type
    
    model_type_requested = request.get("model_type", "").lower()
    
    if not model_type_requested:
        # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏
        rl_models.clear()
        load_all_models()
        return {
            "status": "ok",
            "message": "All models reloaded",
            "loaded_models": list(rl_models.keys()),
            "active_model": current_model_type
        }
    
    # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å
    if model_type_requested in rl_models:
        del rl_models[model_type_requested]
    
    model = load_single_model(model_type_requested)
    if model:
        rl_models[model_type_requested] = model
        # –ï—Å–ª–∏ —ç—Ç–æ –±—ã–ª–∞ –∞–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å, –æ–±–Ω–æ–≤–ª—è–µ–º
        if model_type_requested == current_model_type:
            current_model_type = model_type_requested
        logger.info(f"Reloaded {model_type_requested.upper()} model")
        return {
            "status": "ok",
            "message": f"{model_type_requested.upper()} model reloaded",
            "loaded_models": list(rl_models.keys()),
            "active_model": current_model_type
        }
    else:
        raise HTTPException(
            status_code=404,
            detail=f"Model {model_type_requested} not found or failed to load"
        )


if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8001))
    uvicorn.run(app, host="0.0.0.0", port=port)
